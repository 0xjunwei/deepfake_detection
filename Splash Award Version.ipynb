{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "import PIL.Image\n",
    "import socket\n",
    "from skimage import io\n",
    "import face_alignment\n",
    "import glob\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "#sys.path.insert(0, 'C:/Users/limju/libs')\n",
    "from libs import umeyama\n",
    "from libs import landmarkingAligning\n",
    "from libs import landmarkMasking\n",
    "import imutils\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dnn_Net 00000263AFF6A770>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Input the directory of the folder containing the images that the User Wish to Predict on\n",
    "TESTDIR = \"frontal/\"\n",
    "cas_alt2 = cv2.CascadeClassifier(\"model/haarcascade_frontalface_alt2.xml\")\n",
    "prototxt_path = 'model/caffeFaceExtractionModel/deploy.prototxt.txt'\n",
    "caffemodel_path = 'model/caffeFaceExtractionModel/weights.caffemodel'\n",
    "\n",
    "# Read the Model\n",
    "model = cv2.dnn.readNetFromCaffe(prototxt_path, caffemodel_path)\n",
    "# Using this Caffe Model for Facial Detection\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video, frame_rate):\n",
    "    \n",
    "    #Since extracting Frames > remove any images in all folders\n",
    "    image_folder = glob.glob('images/*')\n",
    "    for f in image_folder:\n",
    "        os.remove(f)\n",
    "        \n",
    "    frontal_folder = glob.glob('frontal/*')\n",
    "    for f in frontal_folder:\n",
    "        os.remove(f)\n",
    "    \n",
    "    #folder path to videos\n",
    "    vid = \"videos/\" + video\n",
    "    \n",
    "    selectedVid = cv2.VideoCapture(vid)\n",
    "    success,image = selectedVid.read()\n",
    "    video_name = str(video)\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "\n",
    "    fps = selectedVid.get(cv2.CAP_PROP_FPS)\n",
    "    print(\"FPS : {0}\".format(fps))\n",
    "    frame_count = int(selectedVid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = frame_count/fps\n",
    "    print(\"Duration : {0} seconds\".format(duration))\n",
    "    cNoFrames = int(frame_rate)\n",
    "\n",
    "    while success:\n",
    "        success,image = selectedVid.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        if count1 % cNoFrames == 0:\n",
    "            #folder path to store extract please make sure same path as extract_align_face \n",
    "            cv2.imwrite(\"images/%d.jpg\" % count2, image) \n",
    "            count2 += 1\n",
    "\n",
    "        count1 += 1\n",
    "    print(\"A Total of %d frames were extracted!\" % count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face():\n",
    "    count = 0\n",
    "    #replace directory with full body image folder\n",
    "    for file in tqdm(os.listdir('images')):\n",
    "        # Obtain both the file_name and its extension \n",
    "        file_name, file_extension = os.path.splitext(file)\n",
    "\n",
    "        # Only perform face extraction if it is a proper picture file\n",
    "        if (file_extension in ['.png','.jpg']):\n",
    "            #Replace directory to path of aligned images\n",
    "            image = cv2.imread('images/' + file)\n",
    "\n",
    "            (h, w) = image.shape[:2]\n",
    "            blob = cv2.dnn.blobFromImage(image, 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "            model.setInput(blob)\n",
    "            detections = model.forward()\n",
    "\n",
    "            # Identify each face\n",
    "            for i in range(0, detections.shape[2]):\n",
    "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "                confidence = detections[0, 0, i, 2]\n",
    "                # If confidence > 0.5, save face found as an actual face image\n",
    "                if (confidence > 0.5):\n",
    "\n",
    "                    frame = image[startY:endY, startX:endX]  \n",
    "                    frame = imutils.resize(frame, height=256)\n",
    "                    xBBox = endX - startX \n",
    "\n",
    "                    x_offsetScaled = float((256 - frame.shape[1])/2*xBBox/frame.shape[1])\n",
    "                    frame = image[startY:endY, math.floor(startX-x_offsetScaled):math.ceil(endX+x_offsetScaled)]\n",
    "                    frame = imutils.resize(frame, height=256)\n",
    "\n",
    "                    if (frame.shape[1] != 256):\n",
    "                        frame = frame[0:256,0:256]\n",
    "\n",
    "                    count += 1 \n",
    "                    #Replace directory of path to store aligned, (Path to aligned faces)\n",
    "                    cv2.imwrite('frontal/'+ file, frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only use frontal face images as model was only trained against frontal faces\n",
    "test_data = []\n",
    "def frontal_face_check():\n",
    "    IMG_SIZE = 256\n",
    "    def create_testing_data():  \n",
    "        #path = os.path.join(TESTDIR, \"predict\") # put path to aligned faces\n",
    "        #for img in os.listdir(path):\n",
    "        counter = 0\n",
    "        for img in tqdm(os.listdir('frontal')):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join('frontal/',img))\n",
    "                retval, faces_haar_alt2 = cv2.face.getFacesHAAR(img_array,\"model/haarcascade_frontalface_alt2.xml\")\n",
    "                faces_haar_alt2 = np.squeeze(faces_haar_alt2)\n",
    "                if((faces_haar_alt2.size <= 1) or (faces_haar_alt2.ndim is not 1)):\n",
    "                    continue\n",
    "                else:\n",
    "                    test_data.append(img_array)\n",
    "                    counter += 1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass\n",
    "\n",
    "    create_testing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you parsing image or video? [ image = 1, videos = 2 ]: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.65it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 22.65it/s]\n"
     ]
    }
   ],
   "source": [
    "vid_image = input(\"Are you parsing image or video? [ image = 1, videos = 2 ]: \")\n",
    "vid_image = int(vid_image)\n",
    "if vid_image is 1:\n",
    "    extract_face()\n",
    "    frontal_face_check()\n",
    "    if(test_data) is None:\n",
    "        print(\"Image invalid\")\n",
    "elif vid_image is 2:\n",
    "    try:\n",
    "        video_input = input(\"Name of Video [Key with extension]: \")\n",
    "        frame_rate = input(\"Extract One of out Nth Frames: \")\n",
    "        extract_frames(video_input, frame_rate)\n",
    "        extract_face()\n",
    "        frontal_face_check()\n",
    "    except:\n",
    "        print(\"Input not valid please rerun the cell\")\n",
    "else:\n",
    "    print(\"Option Given not valid please restart program.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 256\n",
    "val_x = np.array(test_data).reshape(-1, IMG_SIZE, IMG_SIZE, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of model to use without extension: splash_model_30epochs\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 250, 250, 32)      4736      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 125, 125, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 125, 125, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 119, 119, 64)      100416    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 59, 59, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 59, 59, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 55, 55, 128)       204928    \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 51, 51, 128)       409728    \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 47, 47, 128)       409728    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 282752)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                18096192  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 19,227,841\n",
      "Trainable params: 19,227,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#loading the model, Insert the model path into the line below\n",
    "model_name = input(\"Name of model to use without extension: \")\n",
    "loaded_model = tf.keras.models.load_model('model/' + model_name + '.h5')\n",
    "\n",
    "#Displaying a summary of the model used\n",
    "loaded_model.summary()\n",
    "\n",
    "#splash_model_30epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 993us/step\n"
     ]
    }
   ],
   "source": [
    "#this line will help use the loaded model to predict against the entire image within the folder\n",
    "predictions = loaded_model.predict(val_x, verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1  is:  real\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if vid_image is 2:\n",
    "    counter = 0\n",
    "    total = 0\n",
    "    for values in predictions:\n",
    "        value = values.astype(int)\n",
    "        total += 1\n",
    "        if value == 1:\n",
    "            counter += 1\n",
    "    percent = (counter/total) * 100\n",
    "    round(percent)\n",
    "    print(\"This video chances of being a deepfake is : \" + str(round(percent,2)) + \"%\")\n",
    "else:\n",
    "    #Predicted Images 1 is for DeepFake, Whereas 0 is for Real Images\n",
    "    counter = 1\n",
    "    for values in predictions:\n",
    "        value = values.astype(int)\n",
    "        print(\"Image\", counter, \" is: \", 'fake' if value is 1 else 'real')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
